{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epUBTaEqb2fE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![giskard_logo.png](https://raw.githubusercontent.com/Giskard-AI/giskard/main/readme/Logo_full_darkgreen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvW0aV-n0aFE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# About Giskard\n",
    "\n",
    "Open-Source CI/CD platform for ML teams. Deliver ML products, better & faster. \n",
    "\n",
    "*   Collaborate faster with feedback from business stakeholders.\n",
    "*   Deploy automated tests to eliminate regressions, errors & biases.\n",
    "\n",
    "üè° [Website](https://giskard.ai/)\n",
    "\n",
    "üìó [Documentation](https://docs.giskard.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqxJYzAU0hKC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Start by creating a ML model üöÄüöÄüöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKSXB9A8X9z4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download the categorized email files from Berkeley."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osMPzLZuXtIJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!wget http://bailando.sims.berkeley.edu/enron/enron_with_categories.tar.gz\n",
    "!tar zxf enron_with_categories.tar.gz\n",
    "!rm enron_with_categories.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76I7Qz8ui2_B",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "raBI-KqFxUJA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import email\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQkpEBbZYJUC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Various imports and the list of categories from http://bailando.sims.berkeley.edu/enron/enron_categories.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Bgkv4U7YOZ6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stoplist = set(stopwords.words('english') + list(punctuation))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# http://bailando.sims.berkeley.edu/enron/enron_categories.txt\n",
    "idx_to_cat = {\n",
    "    1: 'REGULATION',\n",
    "    2: 'INTERNAL',\n",
    "    3: 'INFLUENCE',\n",
    "    4: 'INFLUENCE',\n",
    "    5: 'INFLUENCE',\n",
    "    6: 'CALIFORNIA CRISIS',\n",
    "    7: 'INTERNAL',\n",
    "    8: 'INTERNAL',\n",
    "    9: 'INFLUENCE',\n",
    "    10: 'REGULATION',\n",
    "    11: 'talking points',\n",
    "    12: 'meeting minutes',\n",
    "    13: 'trip reports'}\n",
    "\n",
    "idx_to_cat2 = {\n",
    "    1: 'regulations and regulators (includes price caps)',\n",
    "    2: 'internal projects -- progress and strategy',\n",
    "    3: ' company image -- current',\n",
    "    4: 'company image -- changing / influencing',\n",
    "    5: 'political influence / contributions / contacts',\n",
    "    6: 'california energy crisis / california politics',\n",
    "    7: 'internal company policy',\n",
    "    8: 'internal company operations',\n",
    "    9: 'alliances / partnerships',\n",
    "    10: 'legal advice',\n",
    "    11: 'talking points',\n",
    "    12: 'meeting minutes',\n",
    "    13: 'trip reports'}\n",
    "\n",
    "\n",
    "LABEL_CAT = 3  # we'll be using the 2nd-level category \"Primary topics\" because the two first levels provide categories that are not mutually exclusive. see : https://bailando.berkeley.edu/enron/enron_categories.txt\n",
    "\n",
    "#get_labels returns a dictionary representation of these labels. \n",
    "def get_labels(filename):\n",
    "    with open(filename + '.cats') as f:\n",
    "        labels = defaultdict(dict)\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            line = line.split(',')\n",
    "            top_cat, sub_cat, freq = int(line[0]), int(line[1]), int(line[2])\n",
    "            labels[top_cat][sub_cat] = freq\n",
    "            line = f.readline()\n",
    "    return dict(labels)\n",
    "\n",
    "\n",
    "email_files = [f.replace('.cats', '') for f in glob.glob('enron_with_categories/*/*.cats')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhXid2KuYprZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Build dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpeOLv-vkoMC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns_name = ['Target', 'Subject', 'Content', 'Week_day', 'Year', 'Month', 'Hour', 'Nb_of_forwarded_msg']\n",
    "\n",
    "\n",
    "data = pd.DataFrame(columns=columns_name)\n",
    "\n",
    "for email_file in email_files:\n",
    "    values_to_add = {}\n",
    "\n",
    "    #Target is the sub-category with maximum frequency\n",
    "    if LABEL_CAT in get_labels(email_file):\n",
    "      sub_cat_dict = get_labels(email_file)[LABEL_CAT]\n",
    "      target_int = max(sub_cat_dict, key=sub_cat_dict.get)\n",
    "      values_to_add['Target'] = str(idx_to_cat[target_int])\n",
    "\n",
    "    #Features are metadata from the email object\n",
    "    filename = email_file+'.txt'\n",
    "    with open(filename) as f:\n",
    "\n",
    "      message = email.message_from_string(f.read())\n",
    "  \n",
    "      values_to_add['Subject'] = str(message['Subject'])\n",
    "      values_to_add['Content'] = str(message.get_payload())\n",
    "     \n",
    "      date_time_obj = parser.parse(message['Date'])\n",
    "      values_to_add['Week_day'] = date_time_obj.strftime(\"%A\")\n",
    "      values_to_add['Year'] = date_time_obj.strftime(\"%Y\")\n",
    "      values_to_add['Month'] = date_time_obj.strftime(\"%B\")\n",
    "      values_to_add['Hour'] = int(date_time_obj.strftime(\"%H\"))\n",
    "\n",
    "      # Count number of forwarded mails\n",
    "      number_of_messages = 0\n",
    "      for line in message.get_payload().split('\\n'):\n",
    "        if ('forwarded' in line.lower() or 'original' in line.lower()) and '--' in line:\n",
    "            number_of_messages += 1\n",
    "      values_to_add['Nb_of_forwarded_msg'] = number_of_messages\n",
    "    \n",
    "    row_to_add = pd.Series(values_to_add)\n",
    "    data = data.append(row_to_add, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQ6jximk1_uU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Filter Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLyNIXUZBdPF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#We filter 879 rows (if Primary topics exists (i.e. if coarse genre 1.1 is selected) )\n",
    "data_filtered = data[data[\"Target\"].notnull()]\n",
    "\n",
    "#Exclude target category with very few rows ; 812 rows remains\n",
    "excluded_category = [idx_to_cat[i] for i in [11,12,13]]\n",
    "data_filtered = data_filtered[data_filtered[\"Target\"].isin(excluded_category) == False]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsZFVQ_4e1ad",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training with scikit learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9npFYMBevLJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_transformer = Pipeline([\n",
    "                      ('vect', CountVectorizer(stop_words=stoplist)),\n",
    "                      ('tfidf', TfidfTransformer())\n",
    "                     ])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "      ('text_Mail', text_transformer, \"Content\")\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(max_iter =1000))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmGR6q6ae8fj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Split train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNJJXj9fer8E",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_types={       \n",
    "        'Target': \"category\",\n",
    "        \"Subject\": \"text\",\n",
    "        \"Content\": \"text\",\n",
    "        \"Week_day\": \"category\",\n",
    "        \"Month\": \"category\",\n",
    "        \"Hour\": \"numeric\",\n",
    "        \"Nb_of_forwarded_msg\": \"numeric\",\n",
    "        \"Year\": \"numeric\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7j_n1VoNFkv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_types = {i:column_types[i] for i in column_types if i!=\"Target\"}\n",
    "Y = data_filtered[\"Target\"]\n",
    "X = data_filtered.drop(columns=\"Target\")\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(X, Y,test_size=0.20, random_state = 30, stratify = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0asMlodfBYJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Learning phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMXyHrXYP-Gf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train)\n",
    "print(\"model score: %.3f\" % clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDr9VIs0VYUM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.concat([X_train, Y_train], axis=1)\n",
    "test_data = pd.concat([X_test, Y_test ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBRKmWMpSCi6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Upload the model in Giskard üöÄüöÄüöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dntUMOGnXRpU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Initiate a project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6tyeii8UET7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from giskard.giskard_client import GiskardClient\n",
    "\n",
    "url = \"http://localhost:9000\" # if docker image is running on your local\n",
    "#url = \"http://app.giskard.ai\" # If you want to upload on giskard URL \n",
    "\n",
    "token = \"XXX\" #Find your token in the Admin tab of your app (login: admin; password: admin)\n",
    "\n",
    "client = GiskardClient(url, token)\n",
    "\n",
    "enron = client.create_project(\"enron_demo\", \"Enron Mails Classification\", \"Project to classify enron mails\")\n",
    "\n",
    "#If you've already created a project with the key \"enron_demo\" use\n",
    "#enron = client.get_project(\"enron_demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmVRDxroUy17",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Upload your model and a dataset (see [documentation](https://docs.giskard.ai/start/guides/upload-your-model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zx8OZMqtU2Aa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "enron.upload_model_and_df(\n",
    "    prediction_function=clf.predict_proba, \n",
    "    model_type='classification',\n",
    "    df=test_data, #the dataset you want to use to inspect your model\n",
    "    column_types=column_types, #all the column types of df\n",
    "    target='Target', #the column name in df corresponding to the actual target variable (ground truth).\n",
    "    feature_names=list(feature_types.keys()),#list of the feature names of prediction_function\n",
    "    model_name='logistic_regression_model',\n",
    "    dataset_name='test_data',\n",
    "    classification_labels=clf.classes_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahi2xz54Z-LE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### üåü If you want to upload a dataset without a model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86zsNSZ1aE8k",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For example, let's upload the train set in Giskard, this is key to create drift tests in Giskard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "px888arOaKH3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "enron.upload_df(\n",
    "    df=train_data,\n",
    "    column_types=column_types, #all the column types of df\n",
    "    target=\"Target\", # do not pass this parameter if dataset doesnt contain target column \n",
    "    name=\"train_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWlbdjq-ae74",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also upload new production data to use it as a validatation set for your existing model. In that case, you might not have the ground truth target variable"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Enron Classification .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}