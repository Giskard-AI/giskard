import inspect

import pandas as pd

from ...scanner.llm.testcase import RequirementEvaluator

from . import debug_description_prefix, debug_prefix
from ...datasets.base import Dataset
from ...ml_worker.testing.registry.decorators import test
from ...ml_worker.testing.test_result import TestResult, TestMessage, TestMessageLevel
from ...models.base import BaseModel


@test(
    name="Validate LLM evaluation dataset using GPT-4",
    tags=["llm", "GPT-4"],
    debug_description=debug_description_prefix + "that are <b>failing the evaluation criteria</b>.",
)
def test_llm_response_validation(
    model: BaseModel, dataset: Dataset, evaluation_criteria: str, threshold: float = 0.5, debug: bool = False
):
    """Tests that the rate of generated response is over a threshold for a given test case.

    The generated response will be validated using GPT-4
    using the OPENAI_API_TOKEN stored inside the environment variable.

    Arguments:
        model(BaseModel): The generative model to test.
        dataset(Dataset): The dataset to test the model on.
        evaluation_criteria(str): The test case used to evaluate the response generated by the model
            must be explicit and clear in order to be interpreted properly
            Good assertions
              - The model response must be a JSON valid that respect the following schema
              - The model response must be an apologies with an explanation of the model scope if the question is out of scope (not related to the Pandas Python library)
            Bad assertion
              - A valid json answer
              - Answer to pandas documentation
        threshold(float, optional): The threshold for good response rate, i.e. the min ratio of responses that pass the assertion. Default is 0.50 (50%).
        debug(bool):
            If True and the test fails,
            a dataset will be provided containing the rows that have failed the evaluation criteria
    """
    from ...llm.utils.validate_test_case import validate_test_case_with_reason

    predictions = model.predict(dataset).prediction

    results = validate_test_case_with_reason(model, evaluation_criteria, dataset.df, predictions)
    passed_tests = [res.score >= 3 for res in results]
    metric = len([result for result in passed_tests if result]) / len(predictions)
    passed = bool(metric >= threshold)

    # --- debug ---
    output_ds = None
    if not passed and debug:
        output_ds = dataset.copy()
        output_ds.df = dataset.df.loc[[not test_passed for test_passed in passed_tests]]
        test_name = inspect.stack()[0][3]
        output_ds.name = debug_prefix + test_name
    # ---

    return TestResult(
        actual_slices_size=[len(dataset)],
        metric=metric,
        passed=passed,
        messages=[
            TestMessage(
                type=TestMessageLevel.INFO,
                text=f"""
Prompt intput: {dataset.df.iloc[i].to_dict()}
                
LLM response: {predictions[i]}
                
Score: {results[i].score}/5
                
Reason: {results[i].reason}
                """,
            )
            for i in range(min(len(predictions), 3))
        ],
        output_df=output_ds,
    )


@test(name="Validate LLM single prompt input using GPT-4", tags=["llm", "GPT-4"])
def test_llm_individual_response_validation(
    model: BaseModel, prompt_input: str, evaluation_criteria: str, debug: bool = False
):
    """Tests that the rate of generated response is over a threshold for a given test case.

    The generated response will be validated using GPT-4
    using the OPENAI_API_TOKEN stored inside the environment variable.

    Arguments:
        model(BaseModel): The generative model to test.
        prompt_input(str): The prompt input to test the model on.
        evaluation_criteria(str): The test case used to evaluate the response generated by the model
            must be explicit and clear in order to be interpreted properly
            Good assertions
              - The model response must be a JSON valid that respect the following schema
              - The model response must be an apologies with an explanation of the model scope if the question is out of scope (not related to the Pandas Python library)
            Bad assertion
              - A valid json answer
              - Answer to pandas documentation
        debug(bool):
            If True and the test fails,
            a dataset will be provided containing the rows that have failed the evaluation criteria
    """
    if len(model.meta.feature_names) != 1:
        raise ValueError(
            "LLM individual response validation only work for models having single input, please use LLM response validation using"
        )

    dataset = Dataset(
        pd.DataFrame({model.meta.feature_names[0]: [prompt_input]}),
        name=f'Single entry dataset for "{evaluation_criteria}"',
        column_types={model.meta.feature_names[0]: "text"},
    )

    return test_llm_response_validation(model, dataset, evaluation_criteria, 1.0, debug).execute()


@test(
    name="llm_output_requirement",
    tags=["llm"],
    debug_description=debug_description_prefix + "that are <b>failing the evaluation criteria</b>.",
)
def test_llm_output_requirement(model: BaseModel, dataset: Dataset, requirements: str, debug: bool = False):
    evaluator = RequirementEvaluator(requirements)
    eval_result = evaluator.evaluate(model, dataset)

    output_ds = None

    if eval_result.failed:
        output_ds = Dataset(pd.DataFrame([ex["input_vars"] for ex in eval_result.failure_examples]), validation=False)

    return TestResult(
        passed=eval_result.passed, output_df=output_ds, metric=len(eval_result.success_examples) / len(dataset)
    )
