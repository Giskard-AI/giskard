title: Statistical
id: statistical
order: 2
items:
  - id: right_label
    title: Right Label
    hint: Test if the model returns the right classification label for a slice
    isMultipleDatasets: false
    isGroundTruthRequired: false
    modelTypes:
      - MULTICLASS_CLASSIFICATION
      - BINARY_CLASSIFICATION
    # language=Python
    code: |-
      #region Documentation
      #      Summary: Test if the model returns the right classification label for a slice
      #
      #      Description: The test is passed when the percentage of rows returning the right
      #      classification label is higher than the threshold in a given slice
      #
      #      Example: For a credit scoring model, the test is passed when more than 50%
      #      of people with high-salaries are classified as “non default”
      #
      #      Args:
      #           actual_slice(GiskardDataset):
      #              Slice of the  actual dataset
      #          model(GiskardModel):
      #              Model used to compute the test
      #          classification_label(str):
      #              Classification label you want to test
      #          threshold(float):
      #              Threshold for the percentage of passed rows
      #
      #      Returns:
      #          metrics:
      #              The ratio of rows with the right classification label over the total of rows in the slice
      #          passed:
      #              TRUE if passed_ratio > threshold
      #endregion

      tests.statistical.test_right_label(
        actual_slice=actual_ds.slice(lambda df: df.head(len(df)//2)),
        model=model,
        classification_label='{{CLASSIFICATION LABEL}}',
        threshold=0.5
      )

  - id: output_in_range_clf
    title: Classification Probability in Range
    hint: Test if the model classification probability belongs to the right range for a slice
    isMultipleDatasets: false
    isGroundTruthRequired: false
    modelTypes:
      - MULTICLASS_CLASSIFICATION
      - BINARY_CLASSIFICATION
    # language=Python
    code: |-
      #region Documentation
      #      Summary: Test if the model classification probability belongs to the right range for a slice
      #
      #      Description: The test is passed when the ratio of rows in the right range inside the
      #      slice is higher than the threshold.
      #
      #      Example: For a credit scoring model, the test is passed when more than 50% of
      #      people with high wage have a probability of defaulting between 0 and 0.1
      #
      #      Args:
      #           actual_slice(GiskardDataset):
      #              Slice of the actual dataset
      #          model(GiskardModel):
      #              Model used to compute the test
      #          classification_label(str):
      #              Classification label you want to test
      #          min_range(float):
      #              Minimum probability of occurrence of classification label
      #          max_range(float):
      #              Maximum probability of occurrence of classification label
      #          threshold(float):
      #              Threshold for the percentage of passed rows
      #
      #      Returns:
      #          metrics:
      #              The proportion of rows in the right range inside the slice
      #          passed:
      #              TRUE if metric > threshold
      #endregion

      tests.statistical.test_output_in_range(
        actual_slice=actual_ds.slice(lambda df: df.head(len(df)//2)),
        model=model,
        classification_label='{{CLASSIFICATION LABEL}}',
        min_range=0,
        max_range=0.1,
        threshold=0.5,
      )

  - id: output_in_range_reg
    title: Regression Output in Range
    hint: Test if the predicted output belongs to the right range for a slice
    isMultipleDatasets: false
    isGroundTruthRequired: false
    modelTypes:
      - REGRESSION
    # language=Python
    code: |-
      #region Documentation
      #      Summary: Test if the predicted output belongs to the right range for a slice
      #
      #      Description: The test is passed when the ratio of rows in the right range inside the
      #      slice is higher than the threshold.
      #
      #      Example : The predicted Sale Price of an item falls in a range between 100 and 1000
      #
      #      Args:
      #           actual_slice(GiskardDataset):
      #              Slice of the actual dataset
      #          model(GiskardModel):
      #              Model used to compute the test
      #          min_range(float):
      #              Minimum value of prediction
      #          max_range(float):
      #              Maximum value of prediction
      #          threshold(float):
      #              Threshold for the percentage of passed rows
      #
      #      Returns:
      #          metrics:
      #              The proportion of rows in the right range inside the slice
      #          passed:
      #              TRUE if metric > threshold
      #endregion

      tests.statistical.test_output_in_range(
        actual_slice=actual_ds.slice(lambda df: df.head(len(df)//2)),
        model=model,
        min_range=100,
        max_range=1000,
        threshold=0.5,
      )

  - id: disparate_impact
    title: Disparate Impact
    hint: Tests if the model is biased more towards an unprotected slice of the dataset over a protected slice
    isMultipleDatasets: false
    isGroundTruthRequired: true
    modelTypes:
      - MULTICLASS_CLASSIFICATION
      - BINARY_CLASSIFICATION
    # language=Python
    code: |-
      #region Documentation
      #        Summary: Tests if the model is biased more towards an unprotected slice of the dataset over a protected slice.
      #        Note that this test reflects only a possible bias in the model while being agnostic to any bias in the dataset
      #        it trained on. The Disparate Impact (DI) is only valid for classification models and is computed as the ratio
      #        between the average count of correct predictions for the protected_slice over the unprotected_slice given a
      #        certain positive_outcome.
      #
      #        Description: Calculate the Disparate Impact between a protected and unprotected slice of a dataset. Otherwise
      #        known as the "80 percent" rule, the Disparate Impact determines if a model was having an "adverse impact" on a
      #        protected (or minority in some cases) group.
      #
      #        Example: The rule was originally based on the rates at which job applicants were hired. For example, if XYZ
      #        Company hired 50 percent of the men applying for work in a predominantly male occupation while hiring only 20
      #        percent of the female applicants, one could look at the ratio of those two hiring rates to judge whether there
      #        might be a discrimination problem. The ratio of 20:50 means that the rate of hiring for female applicants is
      #        only 40 percent of the rate of hiring for male applicants. That is, 20 divided by 50 equals
      #        0.40, which is equivalent to 40 percent. Clearly, 40 percent is well below the 80 percent that was arbitrarily
      #        set as an acceptable difference in hiring rates. Therefore, in this example, XYZ Company could have been called
      #        upon to prove that there was a legitimate reason for hiring men at a rate so much higher than the rate of hiring
      #        women.
      #
      #        Args:
      #              gsk_dataset(GiskardDataset):
      #                  Dataset used to compute the test
      #              protected_slice(Callable):
      #                  Slice that defines the protected group from the full dataset given
      #              unprotected_slice(Callable):
      #                  Slice that defines the unprotected group from the full dataset given
      #              model(GiskardModel):
      #                  Model used to compute the test
      #              positive_outcome(str or float):
      #                  The target value that is considered a positive outcome in the dataset
      #              min_threshold(float):
      #                  Threshold below which the DI test is considered to fail, by default 0.8
      #              max_threshold(float):
      #                  Threshold above which the DI test is considered to fail, by default 1.25
      #
      #        Returns:
      #              metric:
      #                  The disparate impact ratio
      #              passed:
      #                  TRUE if the disparate impact ratio > threshold
      #endregion

      tests.statistical.test_disparate_impact(
        gsk_dataset=actual_ds,
        protected_slice=lambda df: df.head(len(df)//2),
        unprotected_slice=lambda df: df.tail(len(df)//2),
        model=model,
        positive_outcome='{{CLASSIFICATION LABEL}}',
        min_threshold=0.8,
        max_threshold=1.25
      )
