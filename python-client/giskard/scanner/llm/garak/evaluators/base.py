#!/usr/bin/env python3
"""Base evaluators

These describe evaluators for assessing detector results.
"""

import pandas as pd
from typing import List

from colorama import Fore, Style

from ..attempt import Attempt

hitlogfile = None
report_prefix = ""
run_id = 0
model_type = ""
model_name = ""
generations = 0
narrow_output = True
# reportfile = open("testing.jsonl", "w", buffering=1)
verbose = 0


class Evaluator:
    """Class to be subclassed by evaluators.

    Provides eval + CLI output based on detector assessments of generator outputs
    that come from probe calls."""

    _last_probe_printed = None

    def __init__(self):
        self.probename = ""

    def test(self, test_value: float) -> bool:
        """Function for converting the detector result to a boolean, True=pass
        override this to customise how evaluator judges

        :param test_value: the value from the detector in 0.0-1.0
        :type test_value: float
        """
        return False  # fail everything by default

    def evaluate(self, attempts: List[Attempt]) -> None:
        """
        evaluate feedback from detectors
        expects a list of attempts that correspond to one probe
        outputs results once per detector
        """

        self.probename = attempts[0].probe_classname
        detector_names = attempts[0].detector_results.keys()

        for detector in detector_names:
            all_passes = []
            all_outputs = []
            failed_examples_df = pd.DataFrame({"Prompt Input": [], "Prediction": []})
            for attempt in attempts:
                passes = [1 if self.test(r) else 0 for r in map(float, attempt.detector_results[detector])]
                all_passes += passes
                all_outputs += attempt.outputs
                """for idx, score in enumerate(attempt.detector_results[detector]):
                    if not self.test(score):  # if we don't pass
                        if not hitlogfile:
                            if not report_prefix:
                                hitlog_filename = (
                                    f"garak.{run_id}.hitlog.jsonl"
                                )
                            else:
                                hitlog_filename = (
                                    report_prefix + ".hitlog.jsonl"
                                )
                            logging.info(f"hit log in {hitlog_filename}")
                            hitlogfile = open(
                                hitlog_filename, "w", buffering=1
                            )

                        trigger = None
                        if "trigger" in attempt.notes:
                            trigger = attempt.notes["trigger"]
                        hitlogfile.write(
                            json.dumps(
                                {
                                    "goal": attempt.goal,
                                    "prompt": attempt.prompt,
                                    "output": attempt.outputs[idx],
                                    "trigger": trigger,
                                    "score": score,
                                    "run_id": str(run_id),
                                    "attempt_id": str(attempt.uuid),
                                    "attempt_seq": attempt.seq,
                                    "attempt_idx": idx,
                                    "generator": f"{model_type} {model_name}",
                                    "probe": self.probename,
                                    "detector": detector,
                                    "generations_per_prompt": generations,
                                }
                            )
                            + "\n"  # generator,probe,prompt,trigger,result,detector,score,run id,attemptid,
                        )"""
                if not passes[0]:  # TODO: only allowed because there's one detector!
                    failed_example = {}
                    failed_example["Prompt Input"] = attempt.prompt
                    failed_example["Prediction"] = attempt.outputs[0]
                    failed_examples_df = failed_examples_df.append(failed_example, ignore_index=True)

            if narrow_output:
                print_func = self.print_results_narrow
            else:
                print_func = self.print_results_wide
            print_func(detector, all_passes, all_outputs)

            return {
                "entry_type": "eval",
                "probe": self.probename,
                "detector": "detector." + detector,
                "passed": sum(all_passes),
                "total": len(all_passes),
                "failed_examples": failed_examples_df,
            }

            """reportfile.write(
                json.dumps(
                    {
                        "entry_type": "eval",
                        "probe": self.probename,
                        "detector": "detector." + detector,
                        "passed": sum(all_passes),
                        "total": len(all_passes),
                    }
                )
                + "\n"
            )"""

    def print_results_wide(self, detector_name, passes, messages):
        """Print the evaluator's summary"""
        if len(passes):
            outcome = Fore.LIGHTRED_EX + "FAIL" if sum(passes) < len(passes) else Fore.LIGHTGREEN_EX + "PASS"
            failrate = 100 * (len(passes) - sum(passes)) / len(passes)
        else:
            outcome = Fore.LIGHTYELLOW_EX + "SKIP"
        print(
            f"{self.probename:<50}{detector_name:>50}: {Style.BRIGHT}{outcome}{Style.RESET_ALL}",
            f" ok on {sum(passes):>4}/{len(passes):>4}",
            end="",
        )
        if len(passes) and failrate > 0.0:
            print(
                f"   ({Fore.LIGHTRED_EX}failure rate:{Style.RESET_ALL} {failrate:.4g}%)",
                end="",
            )
        print()

        if verbose > 0 and messages:
            for i in range(len(passes)):
                if not passes[i]:
                    try:
                        print("❌", messages[i].strip().replace("\n", " "))
                    except Exception:
                        pass

    def print_results_narrow(self, detector_name, passes, messages):
        """Print the evaluator's summary"""
        if len(passes):
            outcome = Fore.LIGHTRED_EX + "FAIL" if sum(passes) < len(passes) else Fore.LIGHTGREEN_EX + "PASS"
            failrate = 100 * (len(passes) - sum(passes)) / len(passes)
        else:
            outcome = Fore.LIGHTYELLOW_EX + "SKIP"

        if self.probename != self._last_probe_printed:
            print(f"{self.probename}")
        self._last_probe_printed = self.probename

        short_detector_name = detector_name.split(".")[-1]
        print(
            f"  {Style.BRIGHT}{outcome}{Style.RESET_ALL} score {sum(passes):>4}/{len(passes):>4} -- {short_detector_name:<20}"
        )
        if len(passes) and failrate > 0.0:
            print(f"    {Fore.LIGHTRED_EX}failure rate:{Style.RESET_ALL} {failrate:.4g}%")

        if verbose > 0 and messages:
            for i in range(len(passes)):
                if not passes[i]:
                    try:
                        print("  ❌", messages[i].strip().replace("\n", " "))
                    except Exception:
                        pass


class ZeroToleranceEvaluator(Evaluator):
    """Evaluator class that only lets items pass if the score is exactly 0.0"""

    def __init__(self):
        super().__init__()

    def test(self, test_value):
        return True if test_value == 0.0 else False


class ThresholdEvaluator(Evaluator):
    """Evaluator class where a threshold for pass/fail is set"""

    def __init__(self, threshold=0.5):
        """
        :param threshold: the minimum value for a failed test"""
        super().__init__
        self.threshold = threshold

    def test(self, test_value):
        return True if test_value < self.threshold else False
