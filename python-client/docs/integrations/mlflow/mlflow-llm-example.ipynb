{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook Example - Text\n",
    "## Detecting LLM vulnerabilities in MLflow with Giskard\n",
    "This example demonstrates how to efficiently scan two LLMs for hidden vulnerabilities using Giskard and interpret the results within MLflow through just a few lines of code. The LLMs used are:\n",
    "\n",
    "| Model          | Description | Max Tokens | Training data   |\n",
    "|----------------| ----------- | ----------- |-----------------|\n",
    "| `text-ada-001` | Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.| 2049 tokens | Up to Oct 2019  |\n",
    "| `text-davinci-001` | Most capable GPT-3 model. Can do any task the other models can do, often with higher quality.| 2049 tokens | Up to Oct 2019  |\n",
    "\n",
    "Based on the following simple prompt:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "prompt = PromptTemplate(template=\"Create a reader comment according to the following article summary: '{text}'\",\n",
    "                        input_variables=[\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will populate 1000 article summaries from the following [dataset](https://github.com/sunnysai12345/News_Summary), that consists of 4515 examples gathered from Hindu, Indian times and Guardian. Time period ranges from February to august 2017."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv')\n",
    "df_sample = pd.DataFrame(df[\"text\"].sample(1000, random_state=11))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the prompt and dataset in place, we are ready to move forward with evaluating and comparing the LLMs. First, make sure to set up your OpenAI API key:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The initial step involves loading the two models using the langchain library. Next, we log the models in mlflow, and finally, we proceed with the evaluation of each LLM separately using the Giskard evaluator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import openai\n",
    "from langchain import llms, LLMChain\n",
    "\n",
    "models = [\"text-ada-001\", \"text-davinci-001\"]\n",
    "\n",
    "for model in models:\n",
    "    llm = llms.OpenAI(openai_api_key=openai.api_key,\n",
    "                      request_timeout=20,\n",
    "                      max_retries=100,\n",
    "                      temperature=0,\n",
    "                      model_name=model)\n",
    "\n",
    "    chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    with mlflow.start_run(run_name=model):\n",
    "        model_uri = mlflow.langchain.log_model(chain, \"langchain\").model_uri\n",
    "        mlflow.evaluate(model=model_uri,\n",
    "                        model_type=\"text\",\n",
    "                        data=df_sample,\n",
    "                        evaluators=\"giskard\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After completing the previous steps, you can run mlflow ui from the directory where the mlruns folder is located, which will enable you to visualize the results. By accessing http://127.0.0.1:5000, you will be presented with the interface. There, you will find the two LLMs logged as separate runs for comparison and analysis.\n",
    "\n",
    "<img src=\"../../assets/integrations/mlflow/llms/table_view.png\">\n",
    "\n",
    "For each LLM, the following artifacts will be logged:\n",
    "<img src=\"../../assets/integrations/mlflow/llms/artifact_comparison.png\">\n",
    "\n",
    "The giskard scan results:\n",
    "<img src=\"../../assets/integrations/mlflow/llms/text-davinci-001-scanresults2.png\">\n",
    "\n",
    "The metrics generated by the scan:\n",
    "<img src=\"../../assets/integrations/mlflow/llms/text-davinci-001-metrics.png\">\n",
    "\n",
    "A scan summary: After each model evaluation, a scan-summary.json file is created, enabling a comparison of vulnerabilities and metrics for each model in the Artifact view.\n",
    "<img src=\"../../assets/integrations/mlflow/llms/scan-summary.png\">\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
