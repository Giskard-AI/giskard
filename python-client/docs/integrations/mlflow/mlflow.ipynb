{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MLflow\n",
    "\n",
    "Giskard, is available as a seamless plug-in with MLflow's `mlflow.evaluate()` API. With this integration,\n",
    "you can effectively log comprehensive vulnerability reports through Giskard's scanning [capabilities](https://docs.giskard.ai/en/latest/guides/scan/index.html) directly onto the\n",
    "MLflow platform. Furthermore, the integration facilitates metric logging, enabling you to compare the performance,\n",
    "robustness, and even ethical bias of various ML models.\n",
    "\n",
    "## Setup\n",
    "The following requirements are necessary to use the plug-in:\n",
    "\n",
    "- Install `mlflow` to access to the `mlflow.evaluate()` API.\n",
    "- Install `giskard` (follow these [instructions](https://docs.giskard.ai/en/latest/guides/installation_library/index.html))\n",
    "to access to the `giskard` evaluator.\n",
    "\n",
    "After completing the installation process, you will be able to observe giskard as part of mlflowâ€™s evaluators:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.models.list_evaluators() # ['default', 'giskard']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example notebook\n",
    "This example demonstrates how to efficiently scan two LLMs for hidden vulnerabilities using Giskard and interpret the results within MLflow through just a few lines of code. The LLMs used are:\n",
    "\n",
    "| Model          | Description | Max Tokens | Training data   |\n",
    "|----------------| ----------- | ----------- |-----------------|\n",
    "| `text-ada-001` | Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.| 2049 tokens | Up to Oct 2019  |\n",
    "| `text-davinci-001` | Most capable GPT-3 model. Can do any task the other models can do, often with higher quality.| 2049 tokens | Up to Oct 2019  |\n",
    "\n",
    "Based on the following simple prompt:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "prompt = PromptTemplate(template=\"Create a reader comment according to the following article summary: '{text}'\",\n",
    "                        input_variables=[\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will populate 1000 article summaries from the following [dataset](https://github.com/sunnysai12345/News_Summary), that consists of 4515 examples gathered from Hindu, Indian times and Guardian. Time period ranges from February to august 2017."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv')\n",
    "df_sample = pd.DataFrame(df[\"text\"].sample(1000, random_state=11))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the prompt and dataset in place, we are ready to move forward with evaluating and comparing the LLMs. First, make sure to set up your OpenAI API key:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The initial step involves loading the two models using the langchain library. Next, we log the models in mlflow, and finally, we proceed with the evaluation of each LLM separately using the Giskard evaluator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import openai\n",
    "from langchain import llms, LLMChain\n",
    "\n",
    "models = [\"text-ada-001\", \"text-davinci-001\"]\n",
    "\n",
    "for model in models:\n",
    "    llm = llms.OpenAI(openai_api_key=openai.api_key,\n",
    "                      request_timeout=20,\n",
    "                      max_retries=100,\n",
    "                      temperature=0,\n",
    "                      model_name=model)\n",
    "\n",
    "    chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    with mlflow.start_run(run_name=model):\n",
    "        model_uri = mlflow.langchain.log_model(chain, \"langchain\").model_uri\n",
    "        mlflow.evaluate(model=model_uri,\n",
    "                        model_type=\"text\",\n",
    "                        data=df_sample,\n",
    "                        evaluators=\"giskard\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After completing these steps, mlflow will generate a folder named `mlruns` that contains all the results. You can run `mlflow ui` from the directory where the `mlruns` folder is located, which will enable you to visualize the results. By accessing http://127.0.0.1:5000, you will be presented with the interface. There, you will find the two LLMs logged as separate runs for comparison and analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PLug-in parameters\n",
    "\n",
    "The configuration of the giskard evaluator can be done entirely through the `evaluator_config` argument that can yield 3 keys:\n",
    "\n",
    "- `model_config`: to be filled according to this [page](https://docs.giskard.ai/en/latest/reference/models/index.html).\n",
    "- `dataset_config`: to be filled according to this [page](https://docs.giskard.ai/en/latest/reference/datasets/index.html).\n",
    "- `scan_config`: to be filled according to this [page](https://docs.giskard.ai/en/latest/reference/scan/index.html).\n",
    "\n",
    "As an example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluator_config = {\"model_config\":   {\"classification_labels\": [\"no\", \"yes\"]},\n",
    "                    \"dataset_config\": {\"name\": \"Articles\"},\n",
    "                    \"scan_config\":    {\"params\": {\"text_perturbation\": {\"num_samples\": 1000}}}}\n",
    "mlflow.evaluate(model=model_uri,\n",
    "                model_type=\"text\",\n",
    "                data=df_sample,\n",
    "                evaluators=\"giskard\",\n",
    "                evaluator_config=evaluator_config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logging giskard objects to MLflow\n",
    "Suppose you've followed our [quickstart](https://docs.giskard.ai/en/latest/getting-started/quickstart.html) guide, wrapped your [dataset](https://docs.giskard.ai/en/latest/guides/wrap_dataset/index.html) and [model](https://docs.giskard.ai/en/latest/guides/wrap_model/index.html), conducted a [scan](https://docs.giskard.ai/en/latest/guides/scan/index.html), generated a [test-suite](https://docs.giskard.ai/en/latest/guides/scan/index.html) as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# See https://docs.giskard.ai/en/latest/guides/wrap_dataset/index.html\n",
    "# giskard_dataset = ...\n",
    "\n",
    "# See https://docs.giskard.ai/en/latest/guides/wrap_model/index.html\n",
    "# giskard_model = ...\n",
    "\n",
    "scan_results = giskard.scan(giskard_model, giskard_dataset)\n",
    "test_suite = results.generate_test_suite(\"My first test suite\")\n",
    "test_suite_results = test_suite.run()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It will be possible to log the following 4 giskard objects into MLflow: a dataset, a model, a scan and test-suite results.\n",
    "\n",
    "### Option 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Option 1 (via the fluent API)\n",
    "with mlflow.start_run() as run:\n",
    "    giskard_model.to_mlflow()\n",
    "    giskard_dataset.to_mlflow()\n",
    "    scan_results.to_mlflow()\n",
    "    test_suite_results.to_mlflow()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Option 2 (via MlflowClient)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment_id = \"0\"\n",
    "run = client.create_run(experiment_id)\n",
    "\n",
    "giskard_model.to_mlflow(client, run.info.run_id)\n",
    "giskard_dataset.to_mlflow(client, run.info.run_id)\n",
    "scan_results.to_mlflow(client, run.info.run_id)\n",
    "test_suite_results.to_mlflow(client, run.info.run_id)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
