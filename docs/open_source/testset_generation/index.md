# ðŸ§° RAG toolset
Retrieval Augmented Generative models (RAGs) combine LLM models and data sources to produce domain-specific language models able to answer precise questions whose answer are available inside a knowledge base. These models are often extremely specialized to a use-case defined by the information present inside the knowledge base. The specialization of the model makes generic evaluations irrelevant to verify the model's behavior (e.g. hallucinations, trustworthiness, etc.). To this end, the Giskard python library provides a toolset dedicated to RAG models that generates question/answer pairs from the knowledge base of the model.

## How does it work?
The automatic testset generation explores the Knowledge Base (KB) of your model and generate questions and answers related to specific topics available inside the KB. Specifically, we randomly select a topic from the KB, then we extract the related excerpts from the KB to build a `reference_context`. Then we generate a `question` along with a `reference_answer` using an LLM (specifically, we use **OpenAI GPT-4**). 

The generated testset contains a list of questions specific to the model's knowledge base. The model should theoretically answer all these questions correctly. Yet, hallucination or imprecise answers can be generated by the model. This testset allows to quantify how frequent these undesired behaviors happen.

### What data are being sent to OpenAI/Azure OpenAI

In order to perform LLM-assisted detectors, we will be sending the following information to OpenAI/Azure OpenAI:

- Data provided in your knowledge base
- Text generated by your model
- Model name and description

### Will the testset generation work in any language?

The testset quality depends on GPT-4 capabilities regarding your model's language. 

## Before starting

Before starting, make sure you have installed the LLM flavor of Giskard:

```bash
pip install "giskard[llm]"
```

To use the RAG testset generation and evaluation tools, you need to have an OpenAI API key. You can set it in your notebook
like this:

:::::::{tab-set}
::::::{tab-item} OpenAI

```python
import os

os.environ["OPENAI_API_KEY"] = "sk-â€¦"
```

::::::
::::::{tab-item} Azure OpenAI

Require `openai>=1.0.0`
Make sure that both the LLM and Embeddings models are both deployed on the Azure endpoint. The default embedding model used by the Giskard client is `text-embedding-ada-002`. 

```python
import os
from giskard.llm import set_llm_model

os.environ['AZURE_OPENAI_API_KEY'] = '...'
os.environ['AZURE_OPENAI_ENDPOINT'] = 'https://xxx.openai.azure.com'
os.environ['OPENAI_API_VERSION'] = '2023-07-01-preview'


# You'll need to provide the name of the model that you've deployed
# Beware, the model provided must be capable of using function calls
set_llm_model('my-gpt-4-model')
```

::::::
:::::::

We are now ready to start.


## Step 1: Format and load your Knowledge Base
The RAG toolset currently only handles knowledge bases as pandas `DataFrame`. If the DataFrame has multiple columns,
they are concatenated automatically. If only some of the columns contains relevant information, you can specify it when building the generator by passing a list of column names to the `knowledge_base_features` argument (see [API Reference](https://docs.giskard.ai/en/latest/reference/rag-toolset/testset_generation.html#giskard.rag.KnowledgeBaseTestsetGenerator)).


```python
knowledge_base_df = pd.read_*("path/to/your/knowledge_base")
feature_names = ["col1", "col2"]
knowledge_base_df["page_content"] = knowledge_base_df[feature_names].apply(" ".join, axis=1)
```

## Step 2: Generate the testset
Once the knowledge base is loaded as a pandas `DataFrame`, you can generate the testset with the 
`KnowledgeBaseTestsetGenerator`. 


```python
from giskard.rag import KnowledgeBaseTestsetGenerator

generator = KnowledgeBaseTestsetGenerator(knowledge_base_df, 
                    model_name="Model name", # Optional, provide a name to your model to get better fitting questions
                    model_description="Description of the model", # Optional, briefly describe the task done by your model
                    knowledge_base_features=["page_content"])

testset = generator.generate_dataset(num_samples=10)
```

## Step 3: Wrap your model
To evaluate your model, you must wrap it as a `giskard.Model`. This step is necessary to ensure a common format for your model and its metadata.You can wrap anything as long as you can represent it in a Python function (for example an API call call to Azure or OpenAI). We also have pre-built wrappers for LangChain objects, or you can create your own wrapper by extending the `giskard.Model` class if you need to wrap a complex object such as a custom-made RAG communicating with a vectorstore.

To do so, you can follow the instructions from the [LLM Scan feature](../scan/scan_llm/index.md#step-1-wrap-your-model) or from the {doc}`Reference API </reference/models/index>`. Make sure that you pass `feature_names = "question"` when wrapping your model, so that it matches the question column of the testset. 

Detailed examples can also be found on our {doc}`LLM tutorials section </tutorials/llm_tutorials/index>`.


## Step 4: Evaluate your model
Once your `testset` is ready, you can evaluate your wrapped model using the `CorrectnessEvaluator`. This can be done directly or through a Giskard test which wraps the evaluator. The `CorrectnessEvaluator` asks a question to the given model and compares the model answer with the reference answer from the testset. Specifically, we use GPT-4 to assess whether the model answer is acceptable given the reference answer. 


:::::::{tab-set}
::::::{tab-item} Direct Evaluation

The `CorrectnessEvaluator` asks all the questions from the testset to your model and generate a `EvaluationResult` object with all samples from the testset split as pass or fail, and the indices of failed samples in the original testset.
```python
from giskard.llm.evaluators import CorrectnessEvaluator

correctness_evaluator = CorrectnessEvaluator()
eval_result, failed_indices = correctness_evaluator.evaluate(giskard_model, testset)
```
::::::
::::::{tab-item} Giskard test
You can also evaluate your model with the `test_llm_correctness` function, which wraps the `CorrectnessEvaluator` and produce a `TestResult` object as all Giskard test functions. The model passes the test if the ratio of correct answer is above the specified threshold. 
```python
from giskard.testing.tests.llm import test_llm_correctness

test_result = test_llm_correctness(giskard_model, testset, threshold=0.8).execute()
```
::::::
:::::::

## What's next?

The questions generated in the testset may have highlighted some vulnerabilities of your model. There are 2 important actions you can take next:

### 1. Generate a test suite from the testset:

Turn the generated testset into an actionable test suite that you can save and reuse in further iterations.

```python
test_suite = testset.to_test_suite("My first test suite")

# You can run the test suite locally to verify that it reproduces the issues
test_suite.run()
```

Jump to the [test customization](https://docs.giskard.ai/en/latest/open_source/customize_tests/index.html) and [test integration](https://docs.giskard.ai/en/latest/open_source/integrate_tests/index.html) sections to find out everything you can do with test suites.

### 2. Upload your test suite to the Giskard Hub to:
* Compare the quality of different models and prompts to decide which one to promote
* Create more tests relevant to your use case, combining input prompts that make your model fail and custome evaluation criteria
* Share results, and collaborate with your team to integrate business feedback

To upload your test suite, you must have created a project on Giskard Hub and instantiated a Giskard Python client. If you haven't done this yet, follow the first steps of [upload your object](https://docs.giskard.ai/en/latest/giskard_hub/upload/index.html#upload-your-object) guide.

Then, upload your test suite like this:
```python
test_suite.upload(giskard_client, project_key)
```

[Here's a demo](https://huggingface.co/spaces/giskardai/giskard) of the Giskard Hub in action.

## Troubleshooting

If you encounter any issues, join our [Discord community](https://discord.gg/fkv7CAr3FE) and ask questions in our #support channel.
